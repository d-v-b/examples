{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up:\n",
    "\n",
    "To run this example, you will need the at least following libraries `numpy`, `scipy`, `dask`, `distributed`, `matplotlib`, and `fish`. All except `fish` can be installed via `conda`, e.g. `conda install dask`. \n",
    "\n",
    "Unfortunately I haven't made `fish` installable with `pip` or `conda`, so for now you have to clone it from github using `git clone https://github.com/d-v-b/fish.git` and add the path to the cloned folder to your $PYTHONPATH environment variable.\n",
    "\n",
    "I use `flika` for visualizing 3D data plane-by-plane. It can be installed via `pip install flika`. This assumes you are running this notebook with a desktop environment, e.g. via NoMachine. Be advised that attempting to use `flika` without a desktop environment will likely crash the notebook. \n",
    "\n",
    "Make sure you have dask-drmaa or dask-jobqueue installed; if not, run `pip install dask-drmaa` or `pip install dask-jobqueue`. Either works for dynamically requesting workers; in this example, I will use `dask-jobqueue`, using a convenience wrapper that can be found here: https://github.com/d-v-b/fish/blob/master/fish/util/distributed.py#L14\n",
    "\n",
    "you will also need some environment variables set. Put this code block in your `~/.bash_profile` file, then run `source .bash_profile`\n",
    "```bash\n",
    "# Export LSF variables, if available.\n",
    "# May not be available when using Linux locally or Windows with Git Bash.\n",
    "if [[ -f /misc/lsf/conf/profile.lsf ]]; then\n",
    "    source /misc/lsf/conf/profile.lsf\n",
    "    export LSB_STDOUT_DIRECT='Y'\n",
    "    export LSB_JOB_REPORT_MAIL='N'\n",
    "    export LSF_DRMAA_LIBRARY_PATH=/misc/sc/lsf-glibc2.3/lib/libdrmaa.so.0.1.1\n",
    "    export DRMAA_LIBRARY_PATH=$LSF_DRMAA_LIBRARY_PATH\n",
    "fi\n",
    "```\n",
    "\n",
    "see https://github.com/d-v-b/bash_profile-janelia/blob/master/.bash_profile for an example bash_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a bunch of stuff \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.array as da\n",
    "from scipy.ndimage.filters import median_filter\n",
    "\n",
    "import flika as flk\n",
    "flk.start_flika()\n",
    "from flika.window import Window as flw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a parallelizable function that applies a 2D shift to an array. we will use this to apply our registration results\n",
    "def shift_yx(im, shifts, block_id):\n",
    "    from scipy.ndimage.interpolation import shift\n",
    "    t = block_id[0]    \n",
    "    return shift(im.astype('float32'), (0,0, *shifts[t]), order=1, cval=100)\n",
    "\n",
    "# parallelizable function that takes an array as input and performs efficient df/f along the 0th axis of the vector\n",
    "def mydff(v, fs_im):\n",
    "    from fish.image.vol import dff\n",
    "    camera_offset = 80\n",
    "    window = 300 * fs_im\n",
    "    percentile = 20\n",
    "    offset = 10    \n",
    "    downsample = 10;\n",
    "    return dff((v - camera_offset).clip(1, None), window, percentile, offset, downsample, axis=0).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the data, start talking to the distributed scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to raw data\n",
    "from fish.image.zds import ZDS\n",
    "base_dir = '/nrs/ahrens/davis/data/spim/raw/20160608/6dpf_cy171xcy221_f1_omr_1_20160608_170933/'\n",
    "\n",
    "# make a ZDS with the path to raw data.\n",
    "dset = ZDS(base_dir)\n",
    "fs_im = dset.metadata['volume_rate']\n",
    "sample = dset.data[0].compute(scheduler='threads')\n",
    "plt.imshow(sample.max(0), cmap='gray', clim=(100,250))\n",
    "print(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the zds has a data property that is a dask array with 1 chunk : file \n",
    "# For this demo, I crop in time and space using slice objects\n",
    "roi = slice(-500, None), slice(20,30), slice(None), slice(None)\n",
    "data = dset.data[roi]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish.util.distributed import get_jobqueue_cluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# to do distributed computation, dask needs an object that lets it talk to the janelia compute cluster\n",
    "# compute clusters use job scheduler software to give users resources; at Janelia, that software is called \n",
    "# LSF and there are 2 dask libraries that can bridge dask with LSF -- dask-jobqueue and dask-drmaa\n",
    "# here I'm using dask-jobqueue via a wrapper I wrote that uses good janelia-specific default settings\n",
    "cluster = get_jobqueue_cluster()\n",
    "\n",
    "# instantiate a dask.distributed.Client object with the cluster object\n",
    "client = Client(cluster)\n",
    "\n",
    "# once we have a client object, it will register itself with dask as the default scheduler, overriding 'threads'. \n",
    "# so calling dask_array.compute() with no scheduler specified will try to use the distributed scheduler,\n",
    "# even if we have no workers requested (in which case, your computation goes nowhere)\n",
    "# we add workers with cluster.start_workers() and remove them (all) with cluster.stop_all_jobs()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed motion correction\n",
    "\n",
    "1. Generate a reference image \n",
    "2. Make a lazy version of a function that estimates a transform to align two images\n",
    "3. Apply that lazy function to all images in the dataset\n",
    "4. Examine (and modify) estimated transform parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import median_filter\n",
    "\n",
    "# make a filtered version of our raw data\n",
    "# we are mapping a function over 4D chunks of data, so the function needs to assume a 4D input\n",
    "data_filt = data.astype('float32').map_blocks(lambda v: median_filter(v, (1,1,5,5)))    \n",
    "\n",
    "# take the mean to form a reference image for registration\n",
    "# 'threads' is the default scheduler, but I set it explicitly for pedagogical purposes\n",
    "anat_ref = data[data.shape[0]//2 + np.arange(-5,5)].compute(scheduler='threads').mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the reference image\n",
    "flw(anat_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I import a function for estimating translation\n",
    "from fish.image.alignment import estimate_translation\n",
    "# Here I import a function that makes other functions lazy\n",
    "from dask import delayed\n",
    "\n",
    "# make a lazy version of my registration function\n",
    "lazyreg = delayed(estimate_translation)\n",
    "\n",
    "# make a lazy version of the reference image\n",
    "ref_mx = da.from_array(anat_ref.max(0), chunks=(-1,-1))\n",
    "\n",
    "# make a list of lazy registration calculations\n",
    "affs = [lazyreg(ref_mx, mx) for mx in data_filt.max(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get 300 workers to estimate the transform parameters. Ideally we only do this once, save the transform parameters\n",
    "# and load those parameters from disk each time we need to register data.\n",
    "cluster.start_workers(300)\n",
    "reg_result = client.compute(affs, sync=True)\n",
    "shifts = -np.array([r.affine[:-1,-1] for r in reg_result])\n",
    "cluster.stop_all_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12,4), dpi=200)\n",
    "shifts_filt = median_filter(shifts, size=(10,1))\n",
    "axs.plot(shifts)\n",
    "axs.plot(shifts_filt, color='k')\n",
    "axs.legend(['dy','dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply registration, downscale, transpose, and estimate Δf/f\n",
    "\n",
    "1. Apply motion-correction \n",
    "2. Apply a median filter and downscale in xy dimensions\n",
    "3. Rechunk the data so that each chunk contains all timepoints for a few pixels\n",
    "4. Apply Δf/f estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift each timepoint and apply a median filter\n",
    "data_txf = data.map_blocks(shift_yx, shifts, dtype='float32').map_blocks(lambda v: median_filter(v, size=(1,1,3,3)))\n",
    "\n",
    "# reduce data size by 16 by downsampling by 4 in x and y \n",
    "ds_xy = 4\n",
    "data_ds = da.coarsen(np.mean, data_txf, {2: ds_xy, 3: ds_xy})\n",
    "\n",
    "# now rechunk the data so that each chunk contains an entire timeseries. \n",
    "rechunked = data_ds.rechunk(chunks=(-1, 'auto', 'auto','auto'))\n",
    "data_dff = rechunked.map_blocks(lambda v: mydff(v, fs_im=fs_im), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster.start_workers(100)\n",
    "# for this example, I'm taking the max projection in Z to keep the result small\n",
    "result = data_dff.max(1).compute()\n",
    "cluster.stop_all_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the peak-weighted peak times\n",
    "from fish.util.plot import depth_project\n",
    "fig, axs = plt.subplots(dpi=200, figsize=(8,8),ncols=2, gridspec_kw={'width_ratios':(20,1)})\n",
    "cmap='rainbow'\n",
    "axs[0].imshow(depth_project(result, clim=(.1,1), mode='max', cmap=cmap)[:,:,:-1])\n",
    "axs[1].imshow(np.arange(result.shape[0]).reshape(-1,1), cmap=cmap, extent=(0,result.shape[0]//20,0,result.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flw(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
