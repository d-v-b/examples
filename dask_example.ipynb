{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a lot of images using dask\n",
    "\n",
    "This introduction to dask outlines how to do the basic preprocessing operations common to analyzing spatiotemporal imaging data. \n",
    "\n",
    "See also this example from the master of dask: http://matthewrocklin.com/blog/work/2017/01/17/dask-images.\n",
    "\n",
    "`dask.array` docs: http://dask.pydata.org/en/latest/array.html  \n",
    "`dask.distributed` docs: http://distributed.readthedocs.io/en/latest/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "from h5py import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fake_h5_data(dims, directory=None):        \n",
    "    \"\"\"\n",
    "    Save a synthetic n-dimensional dataset as multiple .h5 files. Returns a list of the filenames created\n",
    "    \"\"\"\n",
    "    \n",
    "    from numpy.random import randn\n",
    "    from h5py import File\n",
    "    from tempfile import mkdtemp\n",
    "    from os.path import sep\n",
    "    \n",
    "    if directory == None:\n",
    "        directory = mkdtemp() + sep\n",
    "    \n",
    "    fname_spec = 'ind_{0:05d}.h5'    \n",
    "    fnames = [directory + fname_spec.format(ind) for ind in range(dims[0])]\n",
    "    \n",
    "    for fn in fnames:\n",
    "        data = randn(*dims[1:])        \n",
    "        with File(fn) as f:\n",
    "            f['default'] = data\n",
    "    \n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make some fake 4D data and save to disk. Even though the data are synthetic, I will refer to the first axis as \"time\" and the latter axes as \"space\". Our data starts out in the normal basal state for 4D microscopy data: 1 file per timepoint, and each file contains a 3D volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "dims = (10, 100, 100, 100)\n",
    "fnames = make_fake_h5_data(dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we make a dask array, we need to specify how the data / computations will be distributed. This is \n",
    "done with the `chunks` argument to the ``dask.array`` constructor. If chunks is a tuple of ints, e.g. \n",
    "``(1, 3, 3, 3)``, our data will be logically divided into sub-arrays each with the size ``(1, 3, 3, 3)``. \n",
    "\n",
    "If the first axis of our data is time, then this chunking scheme is natural for operations where we want to do spatial operations on each image, like spatial filtering. However, for timeseries operations like baseline normalization, we will need a different chunking arrangement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = dims[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask.array.from_array()` method takes anything that behaves like a numpy array; In this example the data are stored on disk as `.h5` files, but the `from_array()` would work just as well on a function that gave a numpy-style interface to a `.tif` file or a raw binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through fnames, making a list of dask arrays, one for each file\n",
    "tmp = []\n",
    "for fn in fnames:\n",
    "    tmp.append(da.from_array(File(fn,'r')['default'], chunks=chunks))\n",
    "\n",
    "# stack the dask arrays to form a single array.\n",
    "data = da.stack(tmp)\n",
    "\n",
    "# the previous code is equivalent to this one-liner:\n",
    "data = da.stack([da.from_array(File(fn,'r')['default'], chunks=chunks) for fn in fnames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like spark rdds, most operations on dask arrays are lazy. Indexing a dask array will just return another dask array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our data out, we need to tell dask explicitly to compute a result, using the `dask.array.compute()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0,0,0,0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data spans multiple files, the speed of array indexing depends on the axes the index spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## This reads from a single file\n",
    "tmp = data[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This reads a little from all files, so it's slower\n",
    "tmp = data[:,:,:,:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask.array.compute()` uses local resources (threads, by default) to process data. But  we ultimately want to scale our analysis to cluster-level computing resources. This notebook does not require a compute cluster, but dask lets us run local tasks as if we had a compute cluster by using a `dask.distributed.Client` object and constructing it with a `dask.distributed.LocalCluster` object. See https://github.com/dask/dask-drmaa for an example of this same interface applied to a compute cluster.\n",
    "\n",
    "Once we create our `Client` object, the `dask.array.compute()` method will implicitly use the resources associated with out `Client`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "\n",
    "# configure our local cluster to use 1 worker and threads instead of processes. This is not optimized for \n",
    "# performance, just for demonstration.\n",
    "lc = LocalCluster(n_workers=1, processes=False)\n",
    "client = Client(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the client has a link to a dashboard for tracking the progress of your jobs. This dashboard is much \n",
    "# more exciting than the spark status page.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the ``chunks`` property to see how our data is arranged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this chunking scheme, there is 1 chunk per timepoint, and each chunk is an entire 3D image.\n",
    "\n",
    "Now suppose we want to apply an image filter to every image in `data`. Recall that the chunking scheme of `data` is 1 3D image per chunk. So, to map a function to each chunk in our array, we can use the `dask.array.map_blocks()` method. This is just like `rdd.map()` in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import median_filter\n",
    "\n",
    "# each chunk / block is 4D, even though there's nothing in the first axis, \n",
    "# so we need a 4D image filter that does nothing to the first axis\n",
    "filter_size = (1,3,3,3)\n",
    "data_filtered = data.map_blocks(lambda v: median_filter(v, size=filter_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do simple rigid registration on the `data`. This is kind of annoying to do in dask, and there are probably easier ways to do it, but I haven't played around with this much. With synthetic random data this \"registration\" is meaningless, but the algorithm doesn't know that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a reference image:\n",
    "ref = data[0].compute()\n",
    "\n",
    "# define a function that takes an image and estimates how to align it with the reference\n",
    "# here we do simple fourier-based estimation of translations\n",
    "# FYI map_blocks is not really designed for this kind of thing, but it works. This API might change.\n",
    "def reg(im, reference=ref):\n",
    "    from skimage.feature.register_translation import register_translation\n",
    "    from numpy import squeeze\n",
    "    # im will have shape (1,z,y,x), so we use squeeze to make it 3D\n",
    "    \n",
    "    # this part is annoying: we have to make the result explicitly 2D \n",
    "    shifts = register_translation(squeeze(im), reference)[0].reshape(1,-1)\n",
    "    return shifts\n",
    "\n",
    "# Because we are doing something a little funny with our data (turning a 4D array into 3 numbers) \n",
    "# we need to be more explicit in map_blocks about how the data shape will change, so we specify the new\n",
    "# chunks and the axes that will disappear. I don't like this very much and will be happy to find something\n",
    "# simpler.\n",
    "shifts = data.map_blocks(reg, dtype='float', drop_axis=(2,3), chunks=(1,1)).compute()\n",
    "\n",
    "# once we have the shifts, let's apply them to data.\n",
    "# First I define a function for shifting each image by the correct shifts. This function uses a special\n",
    "# keyword argument, block_id, that tells map_blocks to supply a block_id tuple for each block, like \n",
    "# a key in spark. Without the block_id value it would not be possible to apply the correct shift value.\n",
    "def shifter(im, block_id):\n",
    "    from scipy.ndimage.interpolation import shift\n",
    "    time_index = block_id[0]\n",
    "    return shift(im, shifts[time_index])\n",
    "\n",
    "# now we map that shifting function to each image in data using map_blocks. \n",
    "data_shifted = data.map_blocks(shifter, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is registered, we can consider doing some kind of timeseries operation on it, e.g. standardization with a zscore. Our data will need a different chunking scheme -- we will need at most 1 chunk per timeseries, and we can evenly divide the spatial axes into the remaining chunks. There is overhead associated with chunking, so we do not want a single timeseries per chunk, e.g. `chunks=(num_timepoints, 1, 1, 1)`. To reduce the overhead associated with a lot of chunks, we can do something like `chunks=(num_timepoints, dims[0]//10, dims[1]//10, dims[2]//10)`. The official recommendation is 10-100 MB per chunk, so use that as a guideline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chunks = (dims[0], dims[1] // 10, dims[2] // 10, dims[3] // 10)\n",
    "data_rechunked = data_shifted.rechunk(chunks=new_chunks)\n",
    "data_rechunked.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our rechunked array has blocks that contain an entire timeseries, so we can use `map_blocks` to map a function like `scipy.stats.zscore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    from scipy.stats import zscore\n",
    "    # remember that v will be 4D, so tell zscore which axis to work on\n",
    "    return zscore(v, axis=0)\n",
    "\n",
    "data_zscored = data_rechunked.map_blocks(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data are preprocessed (spatially filtered, motion-corrected, temporally filtered) and ready for whatever else you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
